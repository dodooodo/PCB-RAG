{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "import json\n",
    "\n",
    "p = 'data_title_content.json'\n",
    "data = json.load(open(p))\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create data chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cychiou/anaconda3/envs/pcb1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cychiou/anaconda3/envs/pcb1/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "100%|██████████| 50022/50022 [11:51<00:00, 70.33it/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain_core.documents import Document\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "MAX_CHUNK_TOKENS = 512\n",
    "OVERLAP_RATIO = 0.25\n",
    "END_OF_SENTENCE = \"([^。。\\n\\t\\r!！？\\?]+[。。\\n\\t\\r!！？\\?]*)\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    cache_dir='cache_dir'\n",
    "    )\n",
    "docs = []\n",
    "postID = 0\n",
    "dataID = 0\n",
    "for d in tqdm(data['data']):\n",
    "    seqNUM = 0\n",
    "    \n",
    "    title, content = d['title'], d['content']\n",
    "    if type(content) != str:\n",
    "        continue\n",
    "        \n",
    "    content = title + '\\n' + content\n",
    "    sentences = re.split(END_OF_SENTENCE, content)\n",
    "    sentences = [s for s in sentences if s]\n",
    "    content_tokens = tokenizer(sentences, add_special_tokens=False)['input_ids']\n",
    "    \n",
    "    chunk_tokens = []\n",
    "    n_tokens = 0\n",
    "    i = 0\n",
    "    while i < len(content_tokens):\n",
    "        chunk_tokens.append(content_tokens[i])\n",
    "        n_tokens += len(content_tokens[i])\n",
    "        \n",
    "        if i + 1 == len(content_tokens) or n_tokens + len(content_tokens[i+1]) > MAX_CHUNK_TOKENS:\n",
    "            # print(n_tokens)\n",
    "            page_content = ''.join(tokenizer.batch_decode(chunk_tokens, skip_special_tokens=True))\n",
    "            doc = Document(\n",
    "                page_content=page_content, \n",
    "                metadata={'data_id': dataID, 'post_id': postID, 'seq_num': seqNUM}\n",
    "            )\n",
    "            docs.append(doc)\n",
    "            dataID += 1\n",
    "            seqNUM += 1\n",
    "            \n",
    "            if i + 1 < len(content_tokens):\n",
    "                overlap_n_tokens = 0\n",
    "                for tokens in chunk_tokens[::-1]:\n",
    "                    \n",
    "                    overlap_n_tokens += len(tokens)\n",
    "                    if overlap_n_tokens > n_tokens * OVERLAP_RATIO:\n",
    "                        break\n",
    "                    else:\n",
    "                        i -= 1\n",
    "            \n",
    "            chunk_tokens = []\n",
    "            n_tokens = 0\n",
    "            \n",
    "        i += 1\n",
    "            \n",
    "    postID += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cychiou/anaconda3/envs/pcb1/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "def __load_model_embedding():\n",
    "    return HuggingFaceEmbeddings(\n",
    "            model_name = \"BAAI/bge-m3\",\n",
    "            # model_kwargs = {\"device\": \"cuda\"},\n",
    "            # \n",
    "            encode_kwargs = {'normalize_embeddings': True, 'batch_size': 1},\n",
    "            # \n",
    "            cache_folder = 'cache_dir',    \n",
    "        )\n",
    "    \n",
    "embeddings = __load_model_embedding()\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "db.save_local(\"bge-chunk-512-overlap-25%-boundary\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcb1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
